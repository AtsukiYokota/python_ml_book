{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 系列データのモデル化-リカレントニューラルネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章ではリカレントニューラルネットワーク(RNN:Recurrent Neural Network)に焦点を合わせ，系列データのモデル化への応用と，系列データの一種である時系列データを取り上げる．本章では，次の項目を取り上げる．\n",
    "\n",
    "- 系列データの概要\n",
    "- RNN：シーケンスモデルの構築\n",
    "- **長短期記憶**(LSTM)\n",
    "- **T-BPTT**(Truncated Backpropagation Through Time)\n",
    "- シーケンスモデルを構築するためのTensorFlowでの多層RNNの実装\n",
    "- プロジェクト1：RNNによるIMDb映画レビューデータセットの感情分析\n",
    "- プロジェクト2：シェイクスピアの『ハムレット』を使って文字レベルの言語モデルをRNNモデルとして構築\n",
    "- 勾配の発散を回避するための勾配刈り込みの使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 系列データ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNを説明するにあたってまずは系列データの性質から見ていく．\n",
    "\n",
    "より一般的には系列データは**シーケンス**(sequence)と呼ばれる．\n",
    "ここでは，系列データのユニークな特性を調べることで，他の種類のデータとどのように異なるのかを明らかにする."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 系列データのモデル化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "系列データ(シーケンス)と他の種類のデータとの違いは，シーケンスでは要素が特定の順序で並んでいて，互いに無関係ではないことにある．\n",
    "\n",
    "教師あり機械学習の典型的な機械学習アルゴリズムでは，入力データが**独立同分布**(Independent and Identically Distributed:IID)であることが前提となる．\n",
    "\n",
    "例えば，$n$個のデータサンプル$x^{(1)},x^{(2)},\\cdots,x^{(n)}$がある場合，このデータを機械学習のトレーニングに使用するときの順序は問題ではない．\n",
    "\n",
    "しかし，シーケンスを扱うときには，この前提は成り立たなくなる．当然ながら，順序は重要だからである．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 系列データを表現する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力データでは，シーケンスの要素が互いに依存する順序で並んでいることが裏付けられたとする．\n",
    "\n",
    "本章では，シーケンスを$(x^{(1)},x^{(2)},\\cdots,x^{(T)})$で表すことにする．上付き文字はインスタンスの順序を表しており，シーケンスの長さは$T$である．\n",
    "\n",
    "さて，シーケンスといえば，時系列データである．時系列データでは，各サンプル点$x^{(t)}$が特定の時間$t$に属している．\n",
    "\n",
    "多層パーセプトロン(MLP)や畳み込みニューラルネットワーク(CNN)と言った標準のニューラルネットワークモデルは，入力サンプルの**順序**を処理できない．\n",
    "\n",
    "直感的に言えるのは，そうしたモデルが過去に検出したサンプルの**記憶**を持たない事である．例えば，それらのサンプルはフィードフォワードステップとバックプロパゲーションステップを通過していく．そして，それらの重みは，サンプルが処理される順序とは無関係に更新される．\n",
    "\n",
    "対照的に，リカレントニューラルネットワーク(RNN)の目的は，シーケンスを設計し，モデル化することにある．RNNは過去の情報を記憶しておき，その情報に従って新しい事象を処理できる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### シーケンスモデルの様々なカテゴリ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "シーケンスモデルには，魅力的な応用例がいくつもある．例えば，言語翻訳，画像キャプショニング，テキスト生成などが挙げられる．\n",
    "\n",
    "ただし，適切なモデルを開発するには，シーケンスモデルを構築するための様々な種類のタスクを理解する必要がある．\n",
    "\n",
    "入力データと出力データのどちらかがシーケンスであるとしたら，そのデータは次の3種類のカテゴリのいずれかに分類される．\n",
    "\n",
    "- **多対一**\n",
    "  - 入力データはシーケンスだが，出力は(シーケンスではなく)固定サイズのベクトルである．例えば感情分析では，入力はテキストベースであり，出力はクラスラベルである．\n",
    "- **一対多**\n",
    " - 入力データは標準フォーマットであり，シーケンスではないが，出力はシーケンスである．このカテゴリの一例は画像キャプショニングである．画像キャプショニングでは，入力は画像であり，出力は英語のフレーズである．\n",
    "- **多対多**\n",
    " - 入力データと出力データはどちらもシーケンスである．このカテゴリは，入力と出力が同期するかどうかに基づいて更に分類できる．多対多の**同期モデル**の一例は動画分類である．動画分類では，動画の各フレームがラベル付けされる．多対多の**遅延**モデルの一例は言語翻訳である．例えば，英語をドイツ語に翻訳するときには，英語の文章全体を読み込んで処理したうえでドイツ語に翻訳しなければならない．\n",
    "\n",
    "シーケンスモデルのカテゴリを理解したところで，RNNの構造について見ていく．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## リカレントニューラルネットワーク:シーケンスモデルの構築"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNの構造とデータの流れ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "標準のフィードフォワードニューラルネットワークでは，情報は入力層から隠れ層へ流れ，隠れ層から出力層へ流れる．これに対し，RNNでは，隠れ層の入力は入力層から得られるだけでなく，**1つ前の時間刻みの隠れ層からも得られる**．\n",
    "\n",
    "隠れ層の連続する時間刻みの間を情報が流れることにより，ネットワークが過去の事象に対する記憶を持つことが可能になる．こうした情報の流れは，通常はループ(循環)として表示される．グラフ表記ではこのループは**リカレントエッジ**(recurrent edge)とも呼ばれる．このアーキテクチャ全体を「リカレントニューラルネットワーク」と呼ぶのはそのためである．\n",
    "\n",
    "知っての通り，標準的なニューラルネットワークの隠れユニットが受け取る入力はそれぞれ1つだけである．隠れユニットの入力は，入力層に関連付けられた事前活性化(総入力)である．対照的に，RNNの隠れユニットはそれぞれ，入力層からの事前活性化と，1つ前の時間刻み$t-1$の同じ隠れ層からの活性化という2つの入力を受け取る．\n",
    "\n",
    "最初の時間刻み$t=0$では，隠れユニットはそれぞれ0または小さな乱数で初期化される．次に，$t\\gt0$の時間刻みでは，隠れユニットは2つの場所から入力を受けとる．それらは，現在の時間のデータ点$x^{(t)}$と，1つ前の時間刻み$t-1$の隠れユニットの値$h^{(t-1)}$である．\n",
    "\n",
    "同様に多層RNNの場合は情報の流れを次のようにまとめることが出来る．\n",
    "\n",
    "- **layer=1**\n",
    " - この場合，隠れ層は$h_{1}^{(t)}$で表される．隠れ層の入力は，データ点$x^{(t)}$と，同じ層の1つ前の時間刻みの隠れユニットの値$h_{1}^{(t-1)}$である．\n",
    "- **layer=2**\n",
    " - 2つ目の隠れ層$h_{2}^{(t)}$は，現在の時間刻みの下にある隠れユニット$h_{1}^{(t)}$と，同じ層の1つ前の時間刻みの隠れユニット$h_{2}^{(t-1)}$から入力を受け取る．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 長期的な相互作用の学習と勾配消失・発散問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNのトレーニング手法の1つにBPTT(Backpropagation Through Time)がある．これは，損失関数の勾配を計算するときの乗法係数$\\frac{\\partial h^{(t)}}{\\partial h^{(k)}}$により，いわゆる**勾配消失**(vanishing gradient)問題と**勾配発散**(exploding gradient)問題が発生する．\n",
    "\n",
    "直感的に分かるのは，勾配消失問題や，勾配発散問題を回避するための単純な解決策が，$|w|=1$を保証することによって実現できることである．\n",
    "\n",
    "実際にはこの問題に対する解決策が2つある．\n",
    "\n",
    "- T-BPTT(Truncated Backpropagation Through Time)\n",
    "- 長短期記憶(Long Short-Term Memory:LSTM)\n",
    "\n",
    "T-BPTTは指定されたしきい値の上で勾配を刈り込む．T-BPTTは勾配発散問題を解決できるが，この刈り込みにより，勾配とは逆方向に進んで重みを正しく更新できる時間刻みの数が制限される．\n",
    "\n",
    "一方で，LSTMは勾配消失問題を克服することにより，シーケンスでの長期的な依存関係のモデル化において成功を収めている．\n",
    "\n",
    "以下でLSTMについて少し詳しく見ていく．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTMのユニット"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMの構成要素は**メモリセル**(memory cell)である．メモリセルは基本的には隠れ層を表す．\n",
    "\n",
    "各メモリセルには，勾配消失問題と勾配発散問題を克服するためのリカレントエッジが存在する．\n",
    "\n",
    "先に述べたように，リカレントエッジの望ましい重みは$w=1$である．このリカレントエッジに関連付けられる値は**セル状態**(cell state)と呼ばれる．次の図は，LSTMセルの構造を展開したものである．\n",
    "\n",
    "![LSTMセル](images/lstm_cell.jpg)\n",
    "\n",
    "この図から分かるように，1つ前のセル状態$C^{(t-1)}$が，現在の時間刻みのセル状態を取得するために(重み係数を直接かけることなく)変更される．\n",
    "\n",
    "このメモリセルでの乗法の流れは，次に説明する計算ユニットによって制御される．\n",
    "\n",
    "4つのボックスには，活性化関数(シグモイド関数$\\sigma$, 双曲線正接関数tanh)といちれんの重みが含まれている．\n",
    "これらのボックスでは入力で行列とベクトルの乗算を行うことで，線型結合を適用する．\n",
    "\n",
    "これらの計算つニットとシグモイド活性化関数は**ゲート**(gate)と呼ばれる．ゲートの出力ユニットは$\\odot$を通じて渡される．\n",
    "\n",
    "LSTMセルには，忘却ゲート，入力ゲート，出力ゲートの3種類のゲートが存在する．\n",
    "\n",
    "- **忘却ゲート**($f_{t}$)では，メモリセルを無限に成長させるのではなく，セル状態をリセットできる．実際には，忘却ゲートは通過させる情報と通過させない情報を決定する．$f_{t}$は次の様に計算される． $$f_{t}=\\sigma(W_{xf}x^{(t)}+W_{hf}h^{(t-1)}+b_{f})$$\n",
    "なお，忘却ゲートは最初からLSTMのセルの一部だったわけではなく，最初のモデルを改善するために数年後に追加されたものである．\n",
    "\n",
    "- **入力ゲート**($i_{t}$)と入力ノード($g_{t}$)は，セル状態を更新する役割を果たす．入力ゲートと入力ノードは次のように計算される．\n",
    "$$i_{t}=\\sigma(W_{xi}x^{(t)}+W_{hi}h^{(t-1)}+b_{i}) \\\\ g_{t}=\\tanh(W_{xg}x^{(t)}+W_{hg}h^{(t-1)}+b_{g})$$\n",
    "時間$t$でのセル状態は次のように計算される． $$C^{(t)}=(C^{(t-1)}\\odot f_{t})\\oplus (i_{t}\\odot g_{t})$$\n",
    "\n",
    "- **出力ゲート**($o_{t}$)は，隠れユニットの値の更新方法を決定する．\n",
    "$$o_{t}=\\sigma(W_{xo}x^{(t)}+W_{ho}h^{(t-1)}+b_{o})$$\n",
    "したがって，現在の時間刻みでの隠れユニットは次のように計算される．\n",
    "$$h^{(t)}=o_{t}\\odot \\tanh(C^{(t)})$$\n",
    "\n",
    "LSTMセルの構造とそのベースとなる計算は，かなり複雑に思えるかもしれないが，TensorFlowにはLSTMセルを簡単に定義できるラッパー関数が一通り実装されている．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多層RNNの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### プロジェクト1:IMDb(Internet Movie Database)映画レビューの感情分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感情分析では，文章またはテキスト文書に表明されている意見を分析する．\n",
    "\n",
    "ここでは多対一のアーキテクチャに基づいて感情分析をするための多層RNNを実装する．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### データの準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "映画レビューデータセットはgzipで圧縮されたアーカイブとして[ここ](https://ai.stanford.edu/~amaas/data/sentiment/)で配布されているのでダウンロードして展開しておく．\n",
    "\n",
    "そして，次に示すコードを用いてダウンロードアーカイブに含まれていたテキスト文書を1つのCSVファイルにまとめる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:00\n"
     ]
    }
   ],
   "source": [
    "basepath = '/home/yokota/datasets/aclImdb'\n",
    "labels = {'pos':1, 'neg':0}\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path, file), 'r') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]], ignore_index=True)\n",
    "            pbar.update()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The cast of \"All That\" returns for good humor ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Considering it was made on a low budget, THE D...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Luise Rainer received an Oscar for her perform...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0  The cast of \"All That\" returns for good humor ...  0\n",
       "1  Considering it was made on a low budget, THE D...  1\n",
       "2  Luise Rainer received an Oscar for her perform...  0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 行の順番をシャッフル\n",
    "df = df.sample(frac=True, random_state=0).reset_index(drop=True)\n",
    "\n",
    "# CSVに変換\n",
    "df.to_csv('/home/yokota/datasets/aclImdb/move_data.csv', index=False, encoding='utf-8')\n",
    "\n",
    "csv_filepath = '/home/yokota/datasets/aclImdb/move_data.csv'\n",
    "df = pd.read_csv(csv_filepath, encoding='utf-8')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このdfデータフレームには映画レビューのテキストが含まれる'review'列('0')と0または1のラベルが含まれる'sentiment'('1')の2つの列が含まれている．\n",
    "\n",
    "これらの映画レビューのテキストコンポーネントは，単語のシーケンスである．このため，各シーケンスの単語を処理するRNNモデルを構築し，最終的には文章全体を0または1のクラスに分類したい．\n",
    "\n",
    "ニューラルネットワークへの入力データを準備するには，このデータを数値としてエンコードする必要がある．\n",
    "\n",
    "そのためにはまず，データセット全体から一意な単語を見つけ出す．これにはPythonのset(集合)を利用できるが，こうした大きなデータセットから一意な単語を見つけ出すことはあまり効率的ではないことが分かった．\n",
    "\n",
    "それよりも効率的なのは，collectionsパッケージのCounterを使用する方法である．\n",
    "\n",
    "次のコードではCounterクラスからcountsオブジェクトを作成する．このクラスは，テキストに含まれている一意な単語ごとに出現回数をカウントする．BoW(Bag-of-Words)モデルとは対照的に，このアプリケーションの関心は一意な単語の集まりだけであることに注意する(単語の出現回数には関心がない)．\n",
    "\n",
    "次にマッピングを作成する．このマッピングは，このデータセットの一意な単語をそれぞれ一意な整数にマッピングするディクショナリとして作成する．このディクショナリの名前はword_to_intであり，映画レビューのテキスト全体を数値のリストに変換するために使用できる．検出された一意な単語は出現回数を元にソートされるが，任意の順序を使用したとしても最終的な結果への影響はない．\n",
    "\n",
    "この「テキストを一連の整数に変換する」プロセスのコードは次のようになる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting words occurrences\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:03:55\n",
      "Map reviews to ints\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'the', u'.', u',', u'and', u'a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:04\n"
     ]
    }
   ],
   "source": [
    "# データの前処理：単語を分割し，各単語の出現回数をカウント\n",
    "counts = Counter()\n",
    "pbar = pyprind.ProgBar(len(df['0']), title='Counting words occurrences')\n",
    "\n",
    "for i, review in enumerate(df['0']):\n",
    "    text = ''.join([c if c not in punctuation else ' ' + c + ' '\n",
    "                    for c in review]).lower()\n",
    "    df.loc[i, '0'] = text\n",
    "    pbar.update()\n",
    "    counts.update(text.split())\n",
    "\n",
    "# マッピングを作成：一意な単語をそれぞれ整数にマッピング\n",
    "word_counts = sorted(counts, key=counts.get, reverse=True)\n",
    "print(word_counts[:5])\n",
    "word_to_int = {word: ii for ii, word in enumerate(word_counts, 1)}\n",
    "\n",
    "mapped_reviews = []\n",
    "pbar = pyprind.ProgBar(len(df['0']), title='Map reviews to ints')\n",
    "for review in df['0']:\n",
    "    mapped_reviews.append([word_to_int[word] for word in review.split()])\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで，単語のシーケンスが整数のシーケンスに変換された．ただし，解決しなければならない課題が1つ残っている．\n",
    "\n",
    "現時点では，シーケンスの長さはまちまちである．このRNNアーキテクチャと互換性がある入力データを生成するには全てのシーケンスを同じ長さにする必要がある．\n",
    "\n",
    "そこで，sequence_lengthというパラメータを定義し，200に設定する．シーケンスの長さが200語未満の場合は，シーケンスの左側を0でパディングする．逆に200語を超える場合には，最後の200語だけが使用されるように切り取る．\n",
    "\n",
    "この前処理ステップは，次の2つの手順で実装できる．\n",
    "\n",
    "1. 要素が0の行列を作成する．この行列の各行は，サイズが200のシーケンスに対応する．\n",
    "1. 各シーケンスの単語のインデックスを行列の右側から埋めていく．\n",
    "\n",
    "sequence_lengthは実際にはハイパーパラメータであり，パフォーマンスを最適化するためにチューニングを行うことが出来る．\n",
    "\n",
    "これらの手順を実装して同じ長さのシーケンスを作成するコードは次のようになる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 200 # シーケンスの長さ(RNNの式中のT)\n",
    "sequence = np.zeros((len(mapped_reviews), sequence_length), dtype=int)\n",
    "\n",
    "for i, row in enumerate(mapped_reviews):\n",
    "    review_arr = np.array(row)\n",
    "    sequence[i, -len(row):] = review_arr[-sequence_length:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットの前処理を行った後は，トレーニングデータセットとテストデータセットの分割を行う．\n",
    "\n",
    "このデータセットはすでにシャッフル済みであるため，データセットの前半分をトレーニングに使用し，後半分をテストに使用すれば良い．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sequence[:25000, :]\n",
    "y_train = df.loc[:25000, '1'].values\n",
    "X_test = sequence[25000:, :]\n",
    "y_test = df.loc[25000:, '1'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットを交差検証のために分割したい場合は，データセットの後半分を更に分割することで，一回り小さなテストセットと，ハイパーパラメータを最適化するための検証セットを作成することが出来る．\n",
    "\n",
    "最後に，ヘルパー関数を定義する．この関数は，与えられたデータセット(トレーニングデータセット，テストデータセットの場合がある)をチャンクに分割し，これらのチャンクを反復的に処理するためのジェネレータを返す．こうしたチャンクは**ミニバッチ**(mini-batch)とも呼ばれる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "# ミニバッチを生成する関数を定義\n",
    "def create_batch_generator(x, y=None, batch_size=64):\n",
    "    n_batches = len(x) // batch_size\n",
    "    x = x[:n_batches * batch_size]\n",
    "    if y is not None:\n",
    "        y = y[:n_batches * batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        if y is not None:\n",
    "            yield x[ii:ii+batch_size], y[ii:ii+batch_size]\n",
    "        else:\n",
    "            yield x[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こうしたジェネレータを使用する方法は，メモリの制限に対処するのに非常に効果的である．\n",
    "\n",
    "ニューラルネットワークのトレーニングでは，全てのデータを事前に分割してトレーニングが完了するまでメモリ内で保持するのではなく，データセットをミニバッチに分割する方法が推奨される．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 埋め込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前項では，データの前処理で同じ長さのシーケンスを生成した．これらのシーケンスの要素は，一意な単語の**インデックス**に対応する整数だった．\n",
    "\n",
    "こうした単語のインデックスを入力特徴量に変換する方法は何種類かある．単純な方法の1つはone-hotエンコーディングを適用することでインデックスを0と1のベクトルに変換することである．\n",
    "\n",
    "しかし，入力データ数が大きくなると入力特徴量が大きくなり過ぎでモデルが**次元の呪い**に陥るかもしれない．さらに，これらの特徴量は1つを除いて0であるためかなり疎な特徴量である．\n",
    "\n",
    "より洗練された方法は，実数値の(整数であるとは限らない)要素を持つ固定サイズのベクトルに各単語をマッピングすることである．one-hotエンコーディングのベクトルとは対照的に，有限サイズのベクトルを使って無数の実数を表すことが出来る．理論的には$[-1, 1]$などの区間から実数を無限に抽出できる．\n",
    "\n",
    "これがいわゆる**埋め込み**(embedding)の考え方である．埋め込みは表現学習(目的に適した特徴量ベクトルを学習を通して得る方法，得られる特徴量ベクトルは**分散表現**と呼ばれる)の手法の1つであり，ここでは，データセットの単語を表す顕著な特徴量を自動的に学習するために利用できる．\n",
    "\n",
    "一意な単語の個数がunique\\_words，埋め込みベクトルのサイズがembedding_sizeであるとする．語彙全体を入力特徴量として表すには，embedding_sizeの値をunique\\_wordsよりもかなり小さくすれば良い(embedding\\_size$\\ll$unique\\_words)．\n",
    "\n",
    "埋め込みには，one-hotエンコーディングよりも有利な点が2つある．\n",
    "\n",
    "- 次元の呪いの影響を抑制する特徴空間の次元削減\n",
    "- ニューラルネットワークの埋め込み層がトレーニング可能であることによる顕著な特徴量の抽出\n",
    "\n",
    "TensorFlowにはtf.nn.embedding\\_lookupという効率的な関数が実装されている．この関数は，一意な単語に対応する各整数を，トレーニング可能な行列の行にマッピングする．\n",
    "\n",
    "次に，埋め込み層を実際に作成する方法を見てみる．入力層がtf_xで，対応する語彙のインデックスがtf.int32型で供給されるとすれば，次の2つの手順に従って埋め込み層を作成することが出来る．\n",
    "\n",
    "1. まず，サイズがn\\_words$\\times$embedding\\_sizeの行列をembeddingというテンソル変数として作成する．そして，この行列の要素を$[-1,1]$の浮動小数点型の乱数で初期化する．\n",
    "1. 次に，tf.nn.embedding\\_lookup関数を呼び出し，tf_xの各要素に関連する埋め込み行列の行を特定する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yokota/.local/lib/python2.7/site-packages/requests/__init__.py:83: RequestsDependencyWarning: Old version of cryptography ([1, 2, 3]) may cause slowdown.\n",
      "  warnings.warn(warning, RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tf.Graph()\n",
    "\n",
    "with g.as_default():\n",
    "    tf_x = tf.placeholder(tf.int32, shape=[], name='tf_x')\n",
    "    n_words = len(word_to_int)\n",
    "    embedding_size = 100\n",
    "    \n",
    "    embedding = tf.Variable(tf.random_uniform(shape=(n_words, embedding_size),\n",
    "                            minval=-1, maxval=1))\n",
    "    embed_x = tf.nn.embedding_lookup(embedding, tf_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNNモデルの構築"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNモデルを構築する準備が整ったところで，SentimentRNNクラスを実装してみよう．このクラスには次の4つのメソッドがある．\n",
    "\n",
    "- **コンストラクタ**\n",
    " - モデルのパラメータを全て設定した後，計算グラフを作成し，self.buildメソッドを呼び出して多層RNNモデルを構築する．\n",
    "- **buildメソッド**\n",
    " - 入力データ，入力ラベル，そして隠れ層のドロップアウト設定のキープ率に対応する3つのプレースホルダを宣言する．これらのプレースホルダを宣言した後，埋め込み層を作成し，埋め込み表現を入力として多層RNNを構築する．\n",
    "- **trainメソッド**\n",
    " - 計算グラフを起動するためのTensorFlowセッションを作成し，計算グラフで定義されたコスト関数を最小化するために，ミニバッチを順番に処理しながら，指定された数のエポック数でトレーニングを行う．またチェックポイントとして10エポック後のモデルを保存する．\n",
    "- **predictメソッド**\n",
    " - 新しいセッションを作成し，トレーニングプロセスで保存しておいた最後のチェックポイントを復元し，テストデータで予測値を生成する．\n",
    " \n",
    " このクラスとそのメソッドの実装を次に示す．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(object):\n",
    "    def __init__(self, n_words, seq_len=200,\n",
    "                 lstm_size=256, num_layers=1, batch_size=64,\n",
    "                 learning_rate=0.0001, embed_size=200):\n",
    "        \"\"\"\n",
    "        n_wordsの値は一意な単語の個数に等しくなければならない\n",
    "        (トレーニングの際に1を足すのは長さ200未満のシーケンスを0パディングするためである)\n",
    "        n_wordsは埋め込み層の作成時にembed_sizeハイパーパラメータと共に使用される．\n",
    "        これに対し，seq_len変数は，先の前処理ステップで作成されたシーケンスの長さにしたがって設定されなければならない．\n",
    "        lstm_sizeは，ここで使用しているハイパーパラメータの1つであり，RNNの各層の隠れユニットの個数を決定する．\n",
    "        \"\"\"\n",
    "        self.n_words = n_words\n",
    "        self.seq_len = seq_len\n",
    "        self.lstm_size = lstm_size # 隠れユニットの個数\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(123)\n",
    "            self.build()\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "        \n",
    "    def build(self):\n",
    "        \"\"\"\n",
    "        buildメソッドではまずtf_x,tf_y,tf_keepprobの3つのプレースホルダを作成している．\n",
    "        これらのプレースホルダは入力データを供給するために必要となる．\n",
    "        次に埋め込み層を追加している．この埋め込み層により埋め込み表現embed_xが作成される．\n",
    "        次にRNNモデルとLSTMセルを構築している．この部分は次の3つの手順に分かれている．\n",
    "        1. 多層RNNモデルのセルを定義する．\n",
    "        2. それらのセルの初期状態を定義する．\n",
    "        3. セルとそれらの初期状態に基づいてRNNモデルを作成する．\n",
    "        \"\"\"\n",
    "        tf_x = tf.placeholder(tf.int32, shape=(self.batch_size, self.seq_len), name='tf_x')\n",
    "        tf_y = tf.placeholder(tf.float32, shape=(self.batch_size), name='tf_y')\n",
    "        tf_keepprob = tf.placeholder(tf.float32, name='tf_keepprob')\n",
    "        \n",
    "        # 埋め込み層を作成\n",
    "        embedding = tf.Variable(tf.random_uniform((self.n_words, self.embed_size),\n",
    "                                                  minval=-1, maxval=1), name='embedding')\n",
    "        embed_x = tf.nn.embedding_lookup(embedding, tf_x, name='embeded_x')\n",
    "        \n",
    "        # LSTMセルを定義し，組み上げる\n",
    "        cells = tf.contrib.rnn.MultiRNNCell(\n",
    "            [tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.BasicLSTMCell(self.lstm_size),\n",
    "                                           output_keep_prob=tf_keepprob)\n",
    "            for i in range(self.num_layers)])\n",
    "        \n",
    "        # 初期状態を定義\n",
    "        self.initial_state = cells.zero_state(self.batch_size, tf.float32)\n",
    "        print('  << initial state >>  ', self.initial_state)\n",
    "        \n",
    "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(cells, embed_x, initial_state=self.initial_state)\n",
    "        # 注意：lstm_outputの形状：[batch_size, max_time, cells.output_size]\n",
    "        print('\\n  << lstm_output >>  ', lstm_outputs)\n",
    "        print('\\n  << final state >>  ', self.final_state)\n",
    "        \n",
    "        # RNNの出力の後に全結合層を適用\n",
    "        logits = tf.layers.dense(inputs=lstm_outputs[:, -1],\n",
    "                                 units=1, activation=None,\n",
    "                                 name='logits')\n",
    "        logits = tf.squeeze(logits, name='logits_squeezed')\n",
    "        print('\\n  << logits      >>  ', logits)\n",
    "        \n",
    "        y_proba = tf.nn.sigmoid(logits, name='probabilities')\n",
    "        predictions = {\n",
    "            'probabilities':y_proba,\n",
    "            'labels':tf.cast(tf.round(y_proba), tf.int32, name='labels')\n",
    "        }\n",
    "        print('\\n  << prediction  >>  ', predictions)\n",
    "        \n",
    "        # コスト関数を定義\n",
    "        cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_y,\n",
    "                                                                      logits=logits,\n",
    "                                                                      name='cost'))\n",
    "        # オプティマイザを定義\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.minimize(cost, name='train_op')\n",
    "        \n",
    "    def train(self, X_train, y_train, num_epochs):\n",
    "        \"\"\"\n",
    "        このtrainメソッドの実装では，各エポックの最初に，セルの現在の状態を初期状態にリセットしている．\n",
    "        各ミニバッチの実行では，現在の状態に加えて，データbatch_xとそれらのラベルbatch_yを供給している．\n",
    "        そしてミニバッチの実行の最後に，stateを最終状態に更新している．\n",
    "        最終状態はtf.nn.dynamic_rnn関数から返される．この更新された状態は，次のミニバッチの実行に使用される．\n",
    "        このプロセスが繰り返され，エポックを通じて現在の状態が更新される．\n",
    "        \"\"\"\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "            iteration = 1\n",
    "            for epoch in range(num_epochs):\n",
    "                state = sess.run(self.initial_state)\n",
    "                \n",
    "                for batch_x, batch_y in create_batch_generator(X_train, y_train, self.batch_size):\n",
    "                    feed = {'tf_x:0':batch_x,\n",
    "                            'tf_y:0':batch_y,\n",
    "                            'tf_keepprob:0':0.5,\n",
    "                            self.initial_state: state}\n",
    "                    loss, _, state = sess.run(['cost:0', 'train_op', self.final_state],\n",
    "                                              feed_dict=feed)\n",
    "                    \n",
    "                    if iteration % 20 == 0:\n",
    "                        print('Epoch:{}/{} Iteration:{} | Train loss:{:.5f}'.format(\n",
    "                               epoch + 1, num_epochs, iteration, loss.mean()))\n",
    "                    iteration += 1\n",
    "                \n",
    "                if (epoch + 1) % 10 == 0:\n",
    "                    self.saver.save(sess, 'models/sentiment-{}.ckpt'.format(epoch))\n",
    "    \n",
    "    def predict(self, X_data, return_proba=False):\n",
    "        preds = []\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            self.saver.restore(sess, tf.train.latest_checkpoint('models/'))\n",
    "            test_state = sess.run(self.initial_state)\n",
    "            for ii, batch_x in enumerate(create_batch_generator(X_data, None, batch_size=self.batch_size), 1):\n",
    "                feed = {'tf_x:0':batch_x,\n",
    "                        'tf_keepprob:0':1.0,\n",
    "                        self.initial_state:test_state}\n",
    "            \n",
    "            if return_proba:\n",
    "                pred, test_state = sess.run(['probabilities:0', self.final_state], feed_dict=feed)\n",
    "            else:\n",
    "                pred, test_state = sess.run(['labels:0', self.final_state], feed_dict=feed)\n",
    "            \n",
    "            preds.append(pred)\n",
    "        \n",
    "        return np.concatenate(preds)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SentimentRNNクラスのインスタンス化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SentimentRNNクラスをインスタンス化するには，パラメータを次のように設定する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  << initial state >>   (LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros_1:0' shape=(100, 128) dtype=float32>),)\n",
      "\n",
      "  << lstm_output >>   Tensor(\"rnn/transpose_1:0\", shape=(100, 200, 128), dtype=float32)\n",
      "\n",
      "  << final state >>   (LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_3:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_4:0' shape=(100, 128) dtype=float32>),)\n",
      "\n",
      "  << logits      >>   Tensor(\"logits_squeezed:0\", shape=(100,), dtype=float32)\n",
      "\n",
      "  << prediction  >>   {'probabilities': <tf.Tensor 'probabilities:0' shape=(100,) dtype=float32>, 'labels': <tf.Tensor 'labels:0' shape=(100,) dtype=int32>}\n"
     ]
    }
   ],
   "source": [
    "n_words = max(list(word_to_int.values())) + 1\n",
    "\n",
    "rnn = SentimentRNN(n_words=n_words,\n",
    "                   seq_len=sequence_length,\n",
    "                   embed_size=256,\n",
    "                   lstm_size=128,\n",
    "                   num_layers=1,\n",
    "                   batch_size=100,\n",
    "                   learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは単層RNNを使用するためにnum_layer=1を指定していることが分かる．ただし，この実装ではnum_layerに1よりも大きな値を設定すれば，多層RNNの作成が可能である．\n",
    "\n",
    "ここで検討するデータセットは小さいため，トレーニングデータが過学習する可能性が低い単層RNNのほうが，未知のデータにうまく汎化することが考えられる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 感情分析RNNモデルのトレーニングと最適化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に，rnn.trainメソッドを呼び出すことで，このRNNモデルのトレーニングを行うことができる．このRNNモデルで40エポックのトレーニングを行うコードは次のようになる．\n",
    "\n",
    "このトレーニングでは，X_trainに格納された入力データと，y_trainに格納された対応するクラスラベルを使用する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/40 Iteration:20 | Train loss:0.66575\n",
      "Epoch:1/40 Iteration:40 | Train loss:0.64465\n",
      "Epoch:1/40 Iteration:60 | Train loss:0.66747\n",
      "Epoch:1/40 Iteration:80 | Train loss:0.61411\n",
      "Epoch:1/40 Iteration:100 | Train loss:0.62158\n",
      "Epoch:1/40 Iteration:120 | Train loss:0.50695\n",
      "Epoch:1/40 Iteration:140 | Train loss:0.52379\n",
      "Epoch:1/40 Iteration:160 | Train loss:0.51526\n",
      "Epoch:1/40 Iteration:180 | Train loss:0.52912\n",
      "Epoch:1/40 Iteration:200 | Train loss:0.52265\n",
      "Epoch:1/40 Iteration:220 | Train loss:0.49093\n",
      "Epoch:1/40 Iteration:240 | Train loss:0.48851\n",
      "Epoch:2/40 Iteration:260 | Train loss:0.51594\n",
      "Epoch:2/40 Iteration:280 | Train loss:0.40090\n",
      "Epoch:2/40 Iteration:300 | Train loss:0.28087\n",
      "Epoch:2/40 Iteration:320 | Train loss:0.46239\n",
      "Epoch:2/40 Iteration:340 | Train loss:0.47271\n",
      "Epoch:2/40 Iteration:360 | Train loss:0.29402\n",
      "Epoch:2/40 Iteration:380 | Train loss:0.34497\n",
      "Epoch:2/40 Iteration:400 | Train loss:0.31640\n",
      "Epoch:2/40 Iteration:420 | Train loss:0.27542\n",
      "Epoch:2/40 Iteration:440 | Train loss:0.25068\n",
      "Epoch:2/40 Iteration:460 | Train loss:0.29573\n",
      "Epoch:2/40 Iteration:480 | Train loss:0.31453\n",
      "Epoch:2/40 Iteration:500 | Train loss:0.30023\n",
      "Epoch:3/40 Iteration:520 | Train loss:0.28864\n",
      "Epoch:3/40 Iteration:540 | Train loss:0.23786\n",
      "Epoch:3/40 Iteration:560 | Train loss:0.26940\n",
      "Epoch:3/40 Iteration:580 | Train loss:0.32645\n",
      "Epoch:3/40 Iteration:600 | Train loss:0.24336\n",
      "Epoch:3/40 Iteration:620 | Train loss:0.24461\n",
      "Epoch:3/40 Iteration:640 | Train loss:0.25562\n",
      "Epoch:3/40 Iteration:660 | Train loss:0.28855\n",
      "Epoch:3/40 Iteration:680 | Train loss:0.19916\n",
      "Epoch:3/40 Iteration:700 | Train loss:0.25756\n",
      "Epoch:3/40 Iteration:720 | Train loss:0.16514\n",
      "Epoch:3/40 Iteration:740 | Train loss:0.20358\n",
      "Epoch:4/40 Iteration:760 | Train loss:0.43958\n",
      "Epoch:4/40 Iteration:780 | Train loss:0.20753\n",
      "Epoch:4/40 Iteration:800 | Train loss:0.14215\n",
      "Epoch:4/40 Iteration:820 | Train loss:0.22372\n",
      "Epoch:4/40 Iteration:840 | Train loss:0.21803\n",
      "Epoch:4/40 Iteration:860 | Train loss:0.16219\n",
      "Epoch:4/40 Iteration:880 | Train loss:0.29740\n",
      "Epoch:4/40 Iteration:900 | Train loss:0.14062\n",
      "Epoch:4/40 Iteration:920 | Train loss:0.14698\n",
      "Epoch:4/40 Iteration:940 | Train loss:0.14566\n",
      "Epoch:4/40 Iteration:960 | Train loss:0.20846\n",
      "Epoch:4/40 Iteration:980 | Train loss:0.17728\n",
      "Epoch:4/40 Iteration:1000 | Train loss:0.23035\n",
      "Epoch:5/40 Iteration:1020 | Train loss:0.18184\n",
      "Epoch:5/40 Iteration:1040 | Train loss:0.10425\n",
      "Epoch:5/40 Iteration:1060 | Train loss:0.16380\n",
      "Epoch:5/40 Iteration:1080 | Train loss:0.20609\n",
      "Epoch:5/40 Iteration:1100 | Train loss:0.11903\n",
      "Epoch:5/40 Iteration:1120 | Train loss:0.13117\n",
      "Epoch:5/40 Iteration:1140 | Train loss:0.11506\n",
      "Epoch:5/40 Iteration:1160 | Train loss:0.13310\n",
      "Epoch:5/40 Iteration:1180 | Train loss:0.12794\n",
      "Epoch:5/40 Iteration:1200 | Train loss:0.06422\n",
      "Epoch:5/40 Iteration:1220 | Train loss:0.08054\n",
      "Epoch:5/40 Iteration:1240 | Train loss:0.11906\n",
      "Epoch:6/40 Iteration:1260 | Train loss:0.21960\n",
      "Epoch:6/40 Iteration:1280 | Train loss:0.09214\n",
      "Epoch:6/40 Iteration:1300 | Train loss:0.02830\n",
      "Epoch:6/40 Iteration:1320 | Train loss:0.16419\n",
      "Epoch:6/40 Iteration:1340 | Train loss:0.07946\n",
      "Epoch:6/40 Iteration:1360 | Train loss:0.08751\n",
      "Epoch:6/40 Iteration:1380 | Train loss:0.13060\n",
      "Epoch:6/40 Iteration:1400 | Train loss:0.03346\n",
      "Epoch:6/40 Iteration:1420 | Train loss:0.20809\n",
      "Epoch:6/40 Iteration:1440 | Train loss:0.06366\n",
      "Epoch:6/40 Iteration:1460 | Train loss:0.04635\n",
      "Epoch:6/40 Iteration:1480 | Train loss:0.07556\n",
      "Epoch:6/40 Iteration:1500 | Train loss:0.04117\n",
      "Epoch:7/40 Iteration:1520 | Train loss:0.21352\n",
      "Epoch:7/40 Iteration:1540 | Train loss:0.10587\n",
      "Epoch:7/40 Iteration:1560 | Train loss:0.08621\n",
      "Epoch:7/40 Iteration:1580 | Train loss:0.10948\n",
      "Epoch:7/40 Iteration:1600 | Train loss:0.03534\n",
      "Epoch:7/40 Iteration:1620 | Train loss:0.06424\n",
      "Epoch:7/40 Iteration:1640 | Train loss:0.05076\n",
      "Epoch:7/40 Iteration:1660 | Train loss:0.08266\n",
      "Epoch:7/40 Iteration:1680 | Train loss:0.09161\n",
      "Epoch:7/40 Iteration:1700 | Train loss:0.14903\n",
      "Epoch:7/40 Iteration:1720 | Train loss:0.10648\n",
      "Epoch:7/40 Iteration:1740 | Train loss:0.12408\n",
      "Epoch:8/40 Iteration:1760 | Train loss:0.18739\n",
      "Epoch:8/40 Iteration:1780 | Train loss:0.07404\n",
      "Epoch:8/40 Iteration:1800 | Train loss:0.01560\n",
      "Epoch:8/40 Iteration:1820 | Train loss:0.11120\n",
      "Epoch:8/40 Iteration:1840 | Train loss:0.08890\n",
      "Epoch:8/40 Iteration:1860 | Train loss:0.03939\n",
      "Epoch:8/40 Iteration:1880 | Train loss:0.07159\n",
      "Epoch:8/40 Iteration:1900 | Train loss:0.08511\n",
      "Epoch:8/40 Iteration:1920 | Train loss:0.07031\n",
      "Epoch:8/40 Iteration:1940 | Train loss:0.03866\n",
      "Epoch:8/40 Iteration:1960 | Train loss:0.05768\n",
      "Epoch:8/40 Iteration:1980 | Train loss:0.02756\n",
      "Epoch:8/40 Iteration:2000 | Train loss:0.03915\n",
      "Epoch:9/40 Iteration:2020 | Train loss:0.06397\n",
      "Epoch:9/40 Iteration:2040 | Train loss:0.02553\n",
      "Epoch:9/40 Iteration:2060 | Train loss:0.10746\n",
      "Epoch:9/40 Iteration:2080 | Train loss:0.01362\n",
      "Epoch:9/40 Iteration:2100 | Train loss:0.12034\n",
      "Epoch:9/40 Iteration:2120 | Train loss:0.03708\n",
      "Epoch:9/40 Iteration:2140 | Train loss:0.05239\n",
      "Epoch:9/40 Iteration:2160 | Train loss:0.07470\n",
      "Epoch:9/40 Iteration:2180 | Train loss:0.03482\n",
      "Epoch:9/40 Iteration:2200 | Train loss:0.03055\n",
      "Epoch:9/40 Iteration:2220 | Train loss:0.05227\n",
      "Epoch:9/40 Iteration:2240 | Train loss:0.02251\n",
      "Epoch:10/40 Iteration:2260 | Train loss:0.08232\n",
      "Epoch:10/40 Iteration:2280 | Train loss:0.05602\n",
      "Epoch:10/40 Iteration:2300 | Train loss:0.00785\n",
      "Epoch:10/40 Iteration:2320 | Train loss:0.02422\n",
      "Epoch:10/40 Iteration:2340 | Train loss:0.04995\n",
      "Epoch:10/40 Iteration:2360 | Train loss:0.01077\n",
      "Epoch:10/40 Iteration:2380 | Train loss:0.05936\n",
      "Epoch:10/40 Iteration:2400 | Train loss:0.03621\n",
      "Epoch:10/40 Iteration:2420 | Train loss:0.13834\n",
      "Epoch:10/40 Iteration:2440 | Train loss:0.01235\n",
      "Epoch:10/40 Iteration:2460 | Train loss:0.02378\n",
      "Epoch:10/40 Iteration:2480 | Train loss:0.02560\n",
      "Epoch:10/40 Iteration:2500 | Train loss:0.03905\n",
      "Epoch:11/40 Iteration:2520 | Train loss:0.00746\n",
      "Epoch:11/40 Iteration:2540 | Train loss:0.01215\n",
      "Epoch:11/40 Iteration:2560 | Train loss:0.00683\n",
      "Epoch:11/40 Iteration:2580 | Train loss:0.02880\n",
      "Epoch:11/40 Iteration:2600 | Train loss:0.00933\n",
      "Epoch:11/40 Iteration:2620 | Train loss:0.00138\n",
      "Epoch:11/40 Iteration:2640 | Train loss:0.00349\n",
      "Epoch:11/40 Iteration:2660 | Train loss:0.00635\n",
      "Epoch:11/40 Iteration:2680 | Train loss:0.12406\n",
      "Epoch:11/40 Iteration:2700 | Train loss:0.00524\n",
      "Epoch:11/40 Iteration:2720 | Train loss:0.00744\n",
      "Epoch:11/40 Iteration:2740 | Train loss:0.02205\n",
      "Epoch:12/40 Iteration:2760 | Train loss:0.00760\n",
      "Epoch:12/40 Iteration:2780 | Train loss:0.00811\n",
      "Epoch:12/40 Iteration:2800 | Train loss:0.00381\n",
      "Epoch:12/40 Iteration:2820 | Train loss:0.00571\n",
      "Epoch:12/40 Iteration:2840 | Train loss:0.00709\n",
      "Epoch:12/40 Iteration:2860 | Train loss:0.02874\n",
      "Epoch:12/40 Iteration:2880 | Train loss:0.01735\n",
      "Epoch:12/40 Iteration:2900 | Train loss:0.00887\n",
      "Epoch:12/40 Iteration:2920 | Train loss:0.00159\n",
      "Epoch:12/40 Iteration:2940 | Train loss:0.00164\n",
      "Epoch:12/40 Iteration:2960 | Train loss:0.01324\n",
      "Epoch:12/40 Iteration:2980 | Train loss:0.00327\n",
      "Epoch:12/40 Iteration:3000 | Train loss:0.00236\n",
      "Epoch:13/40 Iteration:3020 | Train loss:0.00760\n",
      "Epoch:13/40 Iteration:3040 | Train loss:0.00131\n",
      "Epoch:13/40 Iteration:3060 | Train loss:0.00148\n",
      "Epoch:13/40 Iteration:3080 | Train loss:0.08928\n",
      "Epoch:13/40 Iteration:3100 | Train loss:0.02920\n",
      "Epoch:13/40 Iteration:3120 | Train loss:0.00313\n",
      "Epoch:13/40 Iteration:3140 | Train loss:0.02792\n",
      "Epoch:13/40 Iteration:3160 | Train loss:0.02307\n",
      "Epoch:13/40 Iteration:3180 | Train loss:0.01216\n",
      "Epoch:13/40 Iteration:3200 | Train loss:0.00471\n",
      "Epoch:13/40 Iteration:3220 | Train loss:0.00278\n",
      "Epoch:13/40 Iteration:3240 | Train loss:0.00487\n",
      "Epoch:14/40 Iteration:3260 | Train loss:0.00705\n",
      "Epoch:14/40 Iteration:3280 | Train loss:0.00152\n",
      "Epoch:14/40 Iteration:3300 | Train loss:0.00162\n",
      "Epoch:14/40 Iteration:3320 | Train loss:0.00392\n",
      "Epoch:14/40 Iteration:3340 | Train loss:0.01476\n",
      "Epoch:14/40 Iteration:3360 | Train loss:0.00413\n",
      "Epoch:14/40 Iteration:3380 | Train loss:0.00230\n",
      "Epoch:14/40 Iteration:3400 | Train loss:0.00294\n",
      "Epoch:14/40 Iteration:3420 | Train loss:0.00249\n",
      "Epoch:14/40 Iteration:3440 | Train loss:0.04877\n",
      "Epoch:14/40 Iteration:3460 | Train loss:0.00712\n",
      "Epoch:14/40 Iteration:3480 | Train loss:0.02637\n",
      "Epoch:14/40 Iteration:3500 | Train loss:0.00877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:15/40 Iteration:3520 | Train loss:0.00849\n",
      "Epoch:15/40 Iteration:3540 | Train loss:0.04937\n",
      "Epoch:15/40 Iteration:3560 | Train loss:0.04561\n",
      "Epoch:15/40 Iteration:3580 | Train loss:0.00219\n",
      "Epoch:15/40 Iteration:3600 | Train loss:0.00670\n",
      "Epoch:15/40 Iteration:3620 | Train loss:0.00847\n",
      "Epoch:15/40 Iteration:3640 | Train loss:0.00857\n",
      "Epoch:15/40 Iteration:3660 | Train loss:0.00133\n",
      "Epoch:15/40 Iteration:3680 | Train loss:0.00694\n",
      "Epoch:15/40 Iteration:3700 | Train loss:0.02613\n",
      "Epoch:15/40 Iteration:3720 | Train loss:0.00454\n",
      "Epoch:15/40 Iteration:3740 | Train loss:0.02586\n",
      "Epoch:16/40 Iteration:3760 | Train loss:0.02627\n",
      "Epoch:16/40 Iteration:3780 | Train loss:0.00119\n",
      "Epoch:16/40 Iteration:3800 | Train loss:0.01931\n",
      "Epoch:16/40 Iteration:3820 | Train loss:0.00654\n",
      "Epoch:16/40 Iteration:3840 | Train loss:0.00458\n",
      "Epoch:16/40 Iteration:3860 | Train loss:0.00105\n",
      "Epoch:16/40 Iteration:3880 | Train loss:0.00306\n",
      "Epoch:16/40 Iteration:3900 | Train loss:0.00793\n",
      "Epoch:16/40 Iteration:3920 | Train loss:0.01726\n",
      "Epoch:16/40 Iteration:3940 | Train loss:0.00053\n",
      "Epoch:16/40 Iteration:3960 | Train loss:0.00176\n",
      "Epoch:16/40 Iteration:3980 | Train loss:0.00206\n",
      "Epoch:16/40 Iteration:4000 | Train loss:0.00147\n",
      "Epoch:17/40 Iteration:4020 | Train loss:0.00125\n",
      "Epoch:17/40 Iteration:4040 | Train loss:0.00114\n",
      "Epoch:17/40 Iteration:4060 | Train loss:0.00099\n",
      "Epoch:17/40 Iteration:4080 | Train loss:0.00081\n",
      "Epoch:17/40 Iteration:4100 | Train loss:0.00792\n",
      "Epoch:17/40 Iteration:4120 | Train loss:0.00061\n",
      "Epoch:17/40 Iteration:4140 | Train loss:0.00343\n",
      "Epoch:17/40 Iteration:4160 | Train loss:0.00344\n",
      "Epoch:17/40 Iteration:4180 | Train loss:0.00211\n",
      "Epoch:17/40 Iteration:4200 | Train loss:0.00047\n",
      "Epoch:17/40 Iteration:4220 | Train loss:0.00235\n",
      "Epoch:17/40 Iteration:4240 | Train loss:0.00170\n",
      "Epoch:18/40 Iteration:4260 | Train loss:0.04941\n",
      "Epoch:18/40 Iteration:4280 | Train loss:0.00054\n",
      "Epoch:18/40 Iteration:4300 | Train loss:0.00054\n",
      "Epoch:18/40 Iteration:4320 | Train loss:0.01267\n",
      "Epoch:18/40 Iteration:4340 | Train loss:0.00275\n",
      "Epoch:18/40 Iteration:4360 | Train loss:0.00163\n",
      "Epoch:18/40 Iteration:4380 | Train loss:0.00476\n",
      "Epoch:18/40 Iteration:4400 | Train loss:0.01420\n",
      "Epoch:18/40 Iteration:4420 | Train loss:0.00941\n",
      "Epoch:18/40 Iteration:4440 | Train loss:0.00093\n",
      "Epoch:18/40 Iteration:4460 | Train loss:0.01730\n",
      "Epoch:18/40 Iteration:4480 | Train loss:0.00934\n",
      "Epoch:18/40 Iteration:4500 | Train loss:0.01316\n",
      "Epoch:19/40 Iteration:4520 | Train loss:0.00193\n",
      "Epoch:19/40 Iteration:4540 | Train loss:0.00818\n",
      "Epoch:19/40 Iteration:4560 | Train loss:0.00179\n",
      "Epoch:19/40 Iteration:4580 | Train loss:0.04531\n",
      "Epoch:19/40 Iteration:4600 | Train loss:0.00053\n",
      "Epoch:19/40 Iteration:4620 | Train loss:0.00452\n",
      "Epoch:19/40 Iteration:4640 | Train loss:0.00454\n",
      "Epoch:19/40 Iteration:4660 | Train loss:0.00234\n",
      "Epoch:19/40 Iteration:4680 | Train loss:0.00120\n",
      "Epoch:19/40 Iteration:4700 | Train loss:0.04811\n",
      "Epoch:19/40 Iteration:4720 | Train loss:0.00411\n",
      "Epoch:19/40 Iteration:4740 | Train loss:0.00149\n",
      "Epoch:20/40 Iteration:4760 | Train loss:0.00315\n",
      "Epoch:20/40 Iteration:4780 | Train loss:0.00326\n",
      "Epoch:20/40 Iteration:4800 | Train loss:0.00137\n",
      "Epoch:20/40 Iteration:4820 | Train loss:0.00072\n",
      "Epoch:20/40 Iteration:4840 | Train loss:0.00178\n",
      "Epoch:20/40 Iteration:4860 | Train loss:0.03359\n",
      "Epoch:20/40 Iteration:4880 | Train loss:0.00431\n",
      "Epoch:20/40 Iteration:4900 | Train loss:0.00928\n",
      "Epoch:20/40 Iteration:4920 | Train loss:0.01000\n",
      "Epoch:20/40 Iteration:4940 | Train loss:0.00364\n",
      "Epoch:20/40 Iteration:4960 | Train loss:0.00239\n",
      "Epoch:20/40 Iteration:4980 | Train loss:0.00220\n",
      "Epoch:20/40 Iteration:5000 | Train loss:0.00294\n",
      "Epoch:21/40 Iteration:5020 | Train loss:0.00389\n",
      "Epoch:21/40 Iteration:5040 | Train loss:0.00154\n",
      "Epoch:21/40 Iteration:5060 | Train loss:0.00038\n",
      "Epoch:21/40 Iteration:5080 | Train loss:0.03258\n",
      "Epoch:21/40 Iteration:5100 | Train loss:0.00289\n",
      "Epoch:21/40 Iteration:5120 | Train loss:0.00154\n",
      "Epoch:21/40 Iteration:5140 | Train loss:0.00703\n",
      "Epoch:21/40 Iteration:5160 | Train loss:0.00541\n",
      "Epoch:21/40 Iteration:5180 | Train loss:0.00075\n",
      "Epoch:21/40 Iteration:5200 | Train loss:0.00033\n",
      "Epoch:21/40 Iteration:5220 | Train loss:0.00108\n",
      "Epoch:21/40 Iteration:5240 | Train loss:0.00107\n",
      "Epoch:22/40 Iteration:5260 | Train loss:0.00138\n",
      "Epoch:22/40 Iteration:5280 | Train loss:0.00050\n",
      "Epoch:22/40 Iteration:5300 | Train loss:0.00038\n",
      "Epoch:22/40 Iteration:5320 | Train loss:0.00082\n",
      "Epoch:22/40 Iteration:5340 | Train loss:0.00614\n",
      "Epoch:22/40 Iteration:5360 | Train loss:0.01321\n",
      "Epoch:22/40 Iteration:5380 | Train loss:0.01148\n",
      "Epoch:22/40 Iteration:5400 | Train loss:0.00165\n",
      "Epoch:22/40 Iteration:5420 | Train loss:0.00061\n",
      "Epoch:22/40 Iteration:5440 | Train loss:0.00610\n",
      "Epoch:22/40 Iteration:5460 | Train loss:0.00076\n",
      "Epoch:22/40 Iteration:5480 | Train loss:0.00094\n",
      "Epoch:22/40 Iteration:5500 | Train loss:0.00045\n",
      "Epoch:23/40 Iteration:5520 | Train loss:0.00518\n",
      "Epoch:23/40 Iteration:5540 | Train loss:0.00145\n",
      "Epoch:23/40 Iteration:5560 | Train loss:0.00018\n",
      "Epoch:23/40 Iteration:5580 | Train loss:0.00365\n",
      "Epoch:23/40 Iteration:5600 | Train loss:0.00538\n",
      "Epoch:23/40 Iteration:5620 | Train loss:0.00170\n",
      "Epoch:23/40 Iteration:5640 | Train loss:0.00560\n",
      "Epoch:23/40 Iteration:5660 | Train loss:0.00124\n",
      "Epoch:23/40 Iteration:5680 | Train loss:0.00057\n",
      "Epoch:23/40 Iteration:5700 | Train loss:0.00032\n",
      "Epoch:23/40 Iteration:5720 | Train loss:0.00108\n",
      "Epoch:23/40 Iteration:5740 | Train loss:0.00109\n",
      "Epoch:24/40 Iteration:5760 | Train loss:0.07454\n",
      "Epoch:24/40 Iteration:5780 | Train loss:0.01136\n",
      "Epoch:24/40 Iteration:5800 | Train loss:0.00084\n",
      "Epoch:24/40 Iteration:5820 | Train loss:0.00053\n",
      "Epoch:24/40 Iteration:5840 | Train loss:0.00216\n",
      "Epoch:24/40 Iteration:5860 | Train loss:0.00103\n",
      "Epoch:24/40 Iteration:5880 | Train loss:0.01099\n",
      "Epoch:24/40 Iteration:5900 | Train loss:0.00150\n",
      "Epoch:24/40 Iteration:5920 | Train loss:0.00035\n",
      "Epoch:24/40 Iteration:5940 | Train loss:0.01198\n",
      "Epoch:24/40 Iteration:5960 | Train loss:0.00051\n",
      "Epoch:24/40 Iteration:5980 | Train loss:0.00655\n",
      "Epoch:24/40 Iteration:6000 | Train loss:0.00576\n",
      "Epoch:25/40 Iteration:6020 | Train loss:0.00170\n",
      "Epoch:25/40 Iteration:6040 | Train loss:0.00019\n",
      "Epoch:25/40 Iteration:6060 | Train loss:0.00031\n",
      "Epoch:25/40 Iteration:6080 | Train loss:0.00029\n",
      "Epoch:25/40 Iteration:6100 | Train loss:0.00058\n",
      "Epoch:25/40 Iteration:6120 | Train loss:0.00026\n",
      "Epoch:25/40 Iteration:6140 | Train loss:0.00660\n",
      "Epoch:25/40 Iteration:6160 | Train loss:0.00107\n",
      "Epoch:25/40 Iteration:6180 | Train loss:0.00064\n",
      "Epoch:25/40 Iteration:6200 | Train loss:0.00018\n",
      "Epoch:25/40 Iteration:6220 | Train loss:0.00049\n",
      "Epoch:25/40 Iteration:6240 | Train loss:0.00027\n",
      "Epoch:26/40 Iteration:6260 | Train loss:0.00515\n",
      "Epoch:26/40 Iteration:6280 | Train loss:0.00038\n",
      "Epoch:26/40 Iteration:6300 | Train loss:0.00020\n",
      "Epoch:26/40 Iteration:6320 | Train loss:0.00404\n",
      "Epoch:26/40 Iteration:6340 | Train loss:0.01625\n",
      "Epoch:26/40 Iteration:6360 | Train loss:0.00040\n",
      "Epoch:26/40 Iteration:6380 | Train loss:0.00095\n",
      "Epoch:26/40 Iteration:6400 | Train loss:0.00324\n",
      "Epoch:26/40 Iteration:6420 | Train loss:0.00263\n",
      "Epoch:26/40 Iteration:6440 | Train loss:0.00028\n",
      "Epoch:26/40 Iteration:6460 | Train loss:0.00803\n",
      "Epoch:26/40 Iteration:6480 | Train loss:0.00125\n",
      "Epoch:26/40 Iteration:6500 | Train loss:0.00032\n",
      "Epoch:27/40 Iteration:6520 | Train loss:0.00783\n",
      "Epoch:27/40 Iteration:6540 | Train loss:0.00069\n",
      "Epoch:27/40 Iteration:6560 | Train loss:0.00064\n",
      "Epoch:27/40 Iteration:6580 | Train loss:0.00078\n",
      "Epoch:27/40 Iteration:6600 | Train loss:0.00781\n",
      "Epoch:27/40 Iteration:6620 | Train loss:0.00629\n",
      "Epoch:27/40 Iteration:6640 | Train loss:0.02568\n",
      "Epoch:27/40 Iteration:6660 | Train loss:0.00173\n",
      "Epoch:27/40 Iteration:6680 | Train loss:0.00040\n",
      "Epoch:27/40 Iteration:6700 | Train loss:0.00039\n",
      "Epoch:27/40 Iteration:6720 | Train loss:0.00119\n",
      "Epoch:27/40 Iteration:6740 | Train loss:0.00049\n",
      "Epoch:28/40 Iteration:6760 | Train loss:0.00189\n",
      "Epoch:28/40 Iteration:6780 | Train loss:0.00427\n",
      "Epoch:28/40 Iteration:6800 | Train loss:0.00061\n",
      "Epoch:28/40 Iteration:6820 | Train loss:0.00167\n",
      "Epoch:28/40 Iteration:6840 | Train loss:0.00207\n",
      "Epoch:28/40 Iteration:6860 | Train loss:0.00078\n",
      "Epoch:28/40 Iteration:6880 | Train loss:0.00095\n",
      "Epoch:28/40 Iteration:6900 | Train loss:0.00326\n",
      "Epoch:28/40 Iteration:6920 | Train loss:0.00037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:28/40 Iteration:6940 | Train loss:0.00023\n",
      "Epoch:28/40 Iteration:6960 | Train loss:0.00206\n",
      "Epoch:28/40 Iteration:6980 | Train loss:0.04161\n",
      "Epoch:28/40 Iteration:7000 | Train loss:0.00074\n",
      "Epoch:29/40 Iteration:7020 | Train loss:0.00157\n",
      "Epoch:29/40 Iteration:7040 | Train loss:0.00026\n",
      "Epoch:29/40 Iteration:7060 | Train loss:0.00014\n",
      "Epoch:29/40 Iteration:7080 | Train loss:0.00035\n",
      "Epoch:29/40 Iteration:7100 | Train loss:0.00129\n",
      "Epoch:29/40 Iteration:7120 | Train loss:0.00005\n",
      "Epoch:29/40 Iteration:7140 | Train loss:0.00034\n",
      "Epoch:29/40 Iteration:7160 | Train loss:0.00033\n",
      "Epoch:29/40 Iteration:7180 | Train loss:0.00012\n",
      "Epoch:29/40 Iteration:7200 | Train loss:0.00023\n",
      "Epoch:29/40 Iteration:7220 | Train loss:0.00083\n",
      "Epoch:29/40 Iteration:7240 | Train loss:0.00011\n",
      "Epoch:30/40 Iteration:7260 | Train loss:0.00269\n",
      "Epoch:30/40 Iteration:7280 | Train loss:0.00031\n",
      "Epoch:30/40 Iteration:7300 | Train loss:0.00052\n",
      "Epoch:30/40 Iteration:7320 | Train loss:0.00077\n",
      "Epoch:30/40 Iteration:7340 | Train loss:0.00874\n",
      "Epoch:30/40 Iteration:7360 | Train loss:0.00014\n",
      "Epoch:30/40 Iteration:7380 | Train loss:0.00039\n",
      "Epoch:30/40 Iteration:7400 | Train loss:0.00621\n",
      "Epoch:30/40 Iteration:7420 | Train loss:0.00034\n",
      "Epoch:30/40 Iteration:7440 | Train loss:0.00095\n",
      "Epoch:30/40 Iteration:7460 | Train loss:0.00124\n",
      "Epoch:30/40 Iteration:7480 | Train loss:0.00252\n",
      "Epoch:30/40 Iteration:7500 | Train loss:0.00037\n",
      "Epoch:31/40 Iteration:7520 | Train loss:0.00214\n",
      "Epoch:31/40 Iteration:7540 | Train loss:0.00022\n",
      "Epoch:31/40 Iteration:7560 | Train loss:0.00044\n",
      "Epoch:31/40 Iteration:7580 | Train loss:0.00039\n",
      "Epoch:31/40 Iteration:7600 | Train loss:0.00151\n",
      "Epoch:31/40 Iteration:7620 | Train loss:0.00004\n",
      "Epoch:31/40 Iteration:7640 | Train loss:0.00010\n",
      "Epoch:31/40 Iteration:7660 | Train loss:0.00131\n",
      "Epoch:31/40 Iteration:7680 | Train loss:0.00170\n",
      "Epoch:31/40 Iteration:7700 | Train loss:0.00398\n",
      "Epoch:31/40 Iteration:7720 | Train loss:0.00016\n",
      "Epoch:31/40 Iteration:7740 | Train loss:0.00055\n",
      "Epoch:32/40 Iteration:7760 | Train loss:0.02954\n",
      "Epoch:32/40 Iteration:7780 | Train loss:0.00067\n",
      "Epoch:32/40 Iteration:7800 | Train loss:0.00057\n",
      "Epoch:32/40 Iteration:7820 | Train loss:0.00014\n",
      "Epoch:32/40 Iteration:7840 | Train loss:0.00022\n",
      "Epoch:32/40 Iteration:7860 | Train loss:0.00097\n",
      "Epoch:32/40 Iteration:7880 | Train loss:0.00094\n",
      "Epoch:32/40 Iteration:7900 | Train loss:0.00048\n",
      "Epoch:32/40 Iteration:7920 | Train loss:0.00032\n",
      "Epoch:32/40 Iteration:7940 | Train loss:0.00011\n",
      "Epoch:32/40 Iteration:7960 | Train loss:0.00035\n",
      "Epoch:32/40 Iteration:7980 | Train loss:0.00045\n",
      "Epoch:32/40 Iteration:8000 | Train loss:0.00008\n",
      "Epoch:33/40 Iteration:8020 | Train loss:0.00123\n",
      "Epoch:33/40 Iteration:8040 | Train loss:0.00010\n",
      "Epoch:33/40 Iteration:8060 | Train loss:0.00017\n",
      "Epoch:33/40 Iteration:8080 | Train loss:0.00013\n",
      "Epoch:33/40 Iteration:8100 | Train loss:0.00014\n",
      "Epoch:33/40 Iteration:8120 | Train loss:0.00010\n",
      "Epoch:33/40 Iteration:8140 | Train loss:0.00006\n",
      "Epoch:33/40 Iteration:8160 | Train loss:0.00177\n",
      "Epoch:33/40 Iteration:8180 | Train loss:0.00028\n",
      "Epoch:33/40 Iteration:8200 | Train loss:0.00005\n",
      "Epoch:33/40 Iteration:8220 | Train loss:0.00026\n",
      "Epoch:33/40 Iteration:8240 | Train loss:0.00008\n",
      "Epoch:34/40 Iteration:8260 | Train loss:0.00002\n",
      "Epoch:34/40 Iteration:8280 | Train loss:0.00243\n",
      "Epoch:34/40 Iteration:8300 | Train loss:0.00009\n",
      "Epoch:34/40 Iteration:8320 | Train loss:0.00014\n",
      "Epoch:34/40 Iteration:8340 | Train loss:0.00020\n",
      "Epoch:34/40 Iteration:8360 | Train loss:0.00003\n",
      "Epoch:34/40 Iteration:8380 | Train loss:0.00012\n",
      "Epoch:34/40 Iteration:8400 | Train loss:0.00007\n",
      "Epoch:34/40 Iteration:8420 | Train loss:0.00005\n",
      "Epoch:34/40 Iteration:8440 | Train loss:0.00003\n",
      "Epoch:34/40 Iteration:8460 | Train loss:0.00059\n",
      "Epoch:34/40 Iteration:8480 | Train loss:0.00013\n",
      "Epoch:34/40 Iteration:8500 | Train loss:0.00004\n",
      "Epoch:35/40 Iteration:8520 | Train loss:0.00044\n",
      "Epoch:35/40 Iteration:8540 | Train loss:0.00003\n",
      "Epoch:35/40 Iteration:8560 | Train loss:0.00004\n",
      "Epoch:35/40 Iteration:8580 | Train loss:0.00002\n",
      "Epoch:35/40 Iteration:8600 | Train loss:0.00008\n",
      "Epoch:35/40 Iteration:8620 | Train loss:0.00003\n",
      "Epoch:35/40 Iteration:8640 | Train loss:0.00011\n",
      "Epoch:35/40 Iteration:8660 | Train loss:0.00012\n",
      "Epoch:35/40 Iteration:8680 | Train loss:0.00012\n",
      "Epoch:35/40 Iteration:8700 | Train loss:0.00001\n",
      "Epoch:35/40 Iteration:8720 | Train loss:0.00004\n",
      "Epoch:35/40 Iteration:8740 | Train loss:0.00003\n",
      "Epoch:36/40 Iteration:8760 | Train loss:0.00005\n",
      "Epoch:36/40 Iteration:8780 | Train loss:0.00004\n",
      "Epoch:36/40 Iteration:8800 | Train loss:0.00015\n",
      "Epoch:36/40 Iteration:8820 | Train loss:0.00004\n",
      "Epoch:36/40 Iteration:8840 | Train loss:0.00006\n",
      "Epoch:36/40 Iteration:8860 | Train loss:0.00017\n",
      "Epoch:36/40 Iteration:8880 | Train loss:0.00008\n",
      "Epoch:36/40 Iteration:8900 | Train loss:0.00019\n",
      "Epoch:36/40 Iteration:8920 | Train loss:0.00002\n",
      "Epoch:36/40 Iteration:8940 | Train loss:0.00006\n",
      "Epoch:36/40 Iteration:8960 | Train loss:0.00008\n",
      "Epoch:36/40 Iteration:8980 | Train loss:0.00025\n",
      "Epoch:36/40 Iteration:9000 | Train loss:0.00002\n",
      "Epoch:37/40 Iteration:9020 | Train loss:0.00016\n",
      "Epoch:37/40 Iteration:9040 | Train loss:0.00002\n",
      "Epoch:37/40 Iteration:9060 | Train loss:0.00003\n",
      "Epoch:37/40 Iteration:9080 | Train loss:0.00004\n",
      "Epoch:37/40 Iteration:9100 | Train loss:0.00005\n",
      "Epoch:37/40 Iteration:9120 | Train loss:0.00002\n",
      "Epoch:37/40 Iteration:9140 | Train loss:0.00005\n",
      "Epoch:37/40 Iteration:9160 | Train loss:0.00007\n",
      "Epoch:37/40 Iteration:9180 | Train loss:0.00001\n",
      "Epoch:37/40 Iteration:9200 | Train loss:0.00007\n",
      "Epoch:37/40 Iteration:9220 | Train loss:0.00002\n",
      "Epoch:37/40 Iteration:9240 | Train loss:0.00005\n",
      "Epoch:38/40 Iteration:9260 | Train loss:0.00003\n",
      "Epoch:38/40 Iteration:9280 | Train loss:0.00013\n",
      "Epoch:38/40 Iteration:9300 | Train loss:0.00001\n",
      "Epoch:38/40 Iteration:9320 | Train loss:0.00003\n",
      "Epoch:38/40 Iteration:9340 | Train loss:0.00005\n",
      "Epoch:38/40 Iteration:9360 | Train loss:0.00006\n",
      "Epoch:38/40 Iteration:9380 | Train loss:0.00005\n",
      "Epoch:38/40 Iteration:9400 | Train loss:0.00045\n",
      "Epoch:38/40 Iteration:9420 | Train loss:0.00002\n",
      "Epoch:38/40 Iteration:9440 | Train loss:0.00001\n",
      "Epoch:38/40 Iteration:9460 | Train loss:0.00432\n",
      "Epoch:38/40 Iteration:9480 | Train loss:0.00005\n",
      "Epoch:38/40 Iteration:9500 | Train loss:0.00003\n",
      "Epoch:39/40 Iteration:9520 | Train loss:0.00008\n",
      "Epoch:39/40 Iteration:9540 | Train loss:0.00001\n",
      "Epoch:39/40 Iteration:9560 | Train loss:0.00001\n",
      "Epoch:39/40 Iteration:9580 | Train loss:0.00001\n",
      "Epoch:39/40 Iteration:9600 | Train loss:0.00013\n",
      "Epoch:39/40 Iteration:9620 | Train loss:0.00002\n",
      "Epoch:39/40 Iteration:9640 | Train loss:0.00007\n",
      "Epoch:39/40 Iteration:9660 | Train loss:0.00008\n",
      "Epoch:39/40 Iteration:9680 | Train loss:0.00003\n",
      "Epoch:39/40 Iteration:9700 | Train loss:0.00001\n",
      "Epoch:39/40 Iteration:9720 | Train loss:0.00004\n",
      "Epoch:39/40 Iteration:9740 | Train loss:0.00002\n",
      "Epoch:40/40 Iteration:9760 | Train loss:0.00001\n",
      "Epoch:40/40 Iteration:9780 | Train loss:0.00004\n",
      "Epoch:40/40 Iteration:9800 | Train loss:0.00001\n",
      "Epoch:40/40 Iteration:9820 | Train loss:0.00002\n",
      "Epoch:40/40 Iteration:9840 | Train loss:0.00003\n",
      "Epoch:40/40 Iteration:9860 | Train loss:0.00001\n",
      "Epoch:40/40 Iteration:9880 | Train loss:0.00016\n",
      "Epoch:40/40 Iteration:9900 | Train loss:0.00106\n",
      "Epoch:40/40 Iteration:9920 | Train loss:0.00001\n",
      "Epoch:40/40 Iteration:9940 | Train loss:0.00001\n",
      "Epoch:40/40 Iteration:9960 | Train loss:0.00004\n",
      "Epoch:40/40 Iteration:9980 | Train loss:0.00001\n",
      "Epoch:40/40 Iteration:10000 | Train loss:0.00002\n"
     ]
    }
   ],
   "source": [
    "rnn.train(X_train, y_train, num_epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/sentiment-39.ckpt\n",
      "test Acc.:50.000%\n"
     ]
    }
   ],
   "source": [
    "preds = rnn.predict(X_test)\n",
    "y_true = y_test[:len(preds)]\n",
    "print('test Acc.:{:.3f}%'.format(100 * (np.sum(preds == y_true) / len(y_true))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/sentiment-39.ckpt\n"
     ]
    }
   ],
   "source": [
    "proba = rnn.predict(X_test, return_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0000000e+00, 8.9112341e-08, 9.9997389e-01, 9.9999952e-01,\n",
       "       5.2278022e-09, 5.9258106e-08, 2.4045349e-07, 1.3626408e-09,\n",
       "       6.0818791e-05, 4.6889502e-07, 4.7616226e-08, 2.7843949e-01,\n",
       "       1.0000000e+00, 1.9271088e-08, 1.0030216e-03, 1.6595300e-04,\n",
       "       1.0000000e+00, 9.9887389e-01, 7.2175670e-09, 1.0000000e+00,\n",
       "       1.0000000e+00, 9.0367990e-08, 9.9999976e-01, 1.0000000e+00,\n",
       "       5.0584714e-09, 9.3793562e-05, 1.0000000e+00, 1.2583264e-06,\n",
       "       1.0000000e+00, 9.9643588e-01, 9.9994409e-01, 9.9999785e-01,\n",
       "       1.0000000e+00, 1.0000000e+00, 9.9999976e-01, 1.0000000e+00,\n",
       "       3.4491585e-08, 3.4103650e-03, 1.0000000e+00, 9.9535710e-01,\n",
       "       1.0000000e+00, 1.0000000e+00, 3.6325947e-08, 9.9999976e-01,\n",
       "       9.9999928e-01, 9.0872443e-10, 1.0000000e+00, 1.0000000e+00,\n",
       "       1.9880648e-09, 9.9999905e-01, 4.9986163e-08, 1.0000000e+00,\n",
       "       1.0000000e+00, 1.0000000e+00, 1.0506316e-06, 9.9990702e-01,\n",
       "       8.6225755e-09, 1.0000000e+00, 6.4979905e-01, 2.3436000e-05,\n",
       "       1.0000000e+00, 9.9998891e-01, 9.8767214e-06, 1.0000000e+00,\n",
       "       1.0000000e+00, 1.0000000e+00, 9.9999785e-01, 1.0000000e+00,\n",
       "       1.0000000e+00, 2.9467909e-08, 4.7647981e-07, 3.0443651e-08,\n",
       "       9.9999988e-01, 2.3992300e-07, 2.5202487e-06, 1.0000000e+00,\n",
       "       8.3147667e-10, 6.5649419e-09, 9.9999976e-01, 1.1315248e-05,\n",
       "       2.4996913e-07, 9.1362456e-03, 1.0000000e+00, 1.0000000e+00,\n",
       "       9.9933332e-01, 1.0000000e+00, 2.8711358e-05, 1.0543568e-07,\n",
       "       4.5658815e-08, 8.5559350e-08, 4.2224038e-08, 9.6068241e-08,\n",
       "       1.0000000e+00, 1.0767366e-07, 1.0000000e+00, 9.9999630e-01,\n",
       "       1.0000000e+00, 8.3409278e-03, 7.1451765e-09, 9.9889129e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
